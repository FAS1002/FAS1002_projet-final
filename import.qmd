---
title: "Importation"
title-block-banner: true
description: | 
  Cette page regroupe la première importation des données à l'état brute ainsi que les manipulation de bases.
# à changer
date: "2022-12-21"
# Modifier les détails que vous voulez
author:
  - name: "Juliette Leblanc"
    # Votre site web perso ou github
    url: https://github.com/JulLeblanc
    # les champs d'affiliation sont optionnels, vous pouvez les
    # comment out en ajoutant un # devant.
    affiliation: FAS1002
    affiliation-url: https://FAS1002.github.io/A22
    # changer pour votre propre orcid id
    # https://orcid.org/ pour vous inscrire.
    # orcid: 0000-0000-0000-0000

# TRUE == Générer une citation pour cette page précise. Pour enlever, mettre false.
citation: true
# Inclure les références que vous utilisez dans vos rapports. Je conseille Zotero pour construire
# ce fichier ou de connecter RStudio directement pour pouvoir citer avec @nom-de-reference.
bibliography: references.bib
---

```{r librairies, echo=FALSE, warning=FALSE,message=FALSE}

library(tidyverse)
library(fs)
library(lubridate)
library(skimr)
library(countrycode)
library(cronR)
library(gsheet)
```


Consignes manquantes:
Pour l’importation des données, vous aurez à les télécharger de plusieurs sources (GitHub, Google Sheet ou Excel). Il y a plusieurs stratégies possibles, à vous de jouer!
○ Cependant, vous devrez faire plus que de seulement importer les données. Puisque tout peut disparaître sur Internet ou être mis à jour sans préavis, vous devrez également télécharger les données. Par contre, il n’est pas optimal de procéder aux téléchargements à chaque fois que nous roulons le code. Les données brutes devront être téléchargées dans le dossier data/raw/ en respectant les conditions:
■ Ainsi, pour les données qui proviennent de Our World in Data, vous devrez développer du code pour télécharger et sauvegarder les fichiers avec la date à laquelle le téléchargement a lieu. La fréquence du téléchargement devrait être quotidienne puisque les données sont mises à jour fréquemment. Pour vous simplifier la tâche, pensez à programmer également la suppression de l’ancien fichier une fois que le nouveau est téléchargé. En d’autres mots, ces données ne devraient être téléchargées qu’une seule fois par jour lorsque votre rapport est produit.
■ Pour les données des autres sources, le principe est le même, mais la fréquence du téléchargement devra être mensuelle, donc à chaque mois seulement.
■ ex: data/raw/owid


#Importer les données

## *Our World in Data*

J'utilise en premier la banque de données du *Co2 and Greenhouse Gaz Emissions*. Celle-ci sera mis à jour à tous les jours. Les données proviennent [d'ici](https://raw.githubusercontent.com/owid/co2-data/master/owid-co2-data.csv).



```{r download co2, cache=TRUE, warning=FALSE, error=FALSE, show_col_types=FALSE}
#option cache=TRUE c'est si le code ne change pas, il ne devrait pas redownloader le fichier

url_owid <- "https://raw.githubusercontent.com/owid/co2-data/master/owid-co2-data.csv"

base_path <- path("data", "raw") # pour que nimporte qui puisse download avec le chemin relatif

fname_owid <- paste(today(), "owid-co2-data.csv", sep = "_") # name the file with today's date

fpath_owid <- path(base_path, fname_owid) 

data_owid <- download.file(url = url_owid, destfile = fpath_owid)

df_owid <- read_csv(fpath_owid,  show_col_types = FALSE)
```

## *Gapminder*

Pour la deuxième banque de données, je fais appel à celle produite par *Gapminder* qui se nomme *Life Expectancy at Birth*. Celle-ci seront re-téléchargé pour une mise à jour à tous les mois. Les données proviennent [d'ici]("https://docs.google.com/spreadsheets/d/1RheSon1-q4vFc3AGyupVPH6ptEByE-VtnjOCselU0PE/edit#gid=176703676")

```{r download lifeexpectancy, cache=TRUE, warning=FALSE, error=FALSE}
url_gapminder <-  gsheet2tbl("https://docs.google.com/spreadsheets/d/1RheSon1-q4vFc3AGyupVPH6ptEByE-VtnjOCselU0PE/edit#gid=176703676")

base_path <- path("data", "raw")

fname_gap <- paste(today(), "gapm.csv", sep = "_")

fpath_gap <- path(base_path, fname_gap)

write_csv(x = url_gapminder, file = fpath_gap)

df_gapminder <- read_csv(fpath_gap, show_col_types = FALSE)
```

# Variable continent

Dans les deux banques de données que nous utilisons, j'ai créé une variable qui regroupe les différents pays en sous-groupe des 5 continents soit : l'Afrique, l'Amérique, l'Asie, l'Europe et l'Océanie. Ceci me permettra de faire des comparaisons plus large en me référant au continent et non au pays.

```{r variable for continents, warning=FALSE, error=FALSE}
df_owid <- df_owid |> 
    mutate(continent = countrycode(sourcevar = df_owid$country,
                                   origin = "country.name",
                                   destination = "continent"))

df_gapminder <- df_gapminder |> 
    mutate(continent = countrycode(sourcevar = df_gapminder$name,
                                   origin = "country.name",
                                   destination = "continent"))
```

# Recoder certaines variables

Dans les banques de données que nous utilisons, des variables qui regroupent le même type d'information n'ont pas nécessairement le même nom.

Notamment, dans la banque de données 
r par uniformiser ceci, notamment en renommant les colonnes des pays pour `country`et pour l'année `year`.

```{r recode variables, warning=FALSE, error=FALSE}
df_gapminder <- df_gapminder |> 
    rename(country = name,
           year = time,
           life_exp = `Life expectancy`)
```


## Exporter

Maintenant qu'une première manipulation des données a été fait, nous pouvons exporter celles-ci comme étant des données traitées dans le dossier `data/processed`.

Les prochaines analyses auront comme point de départ les données traitées, donc qui ont été exportées dans le chemin relatif ci-dessus. 

```{r exporting data to processed, warning=FALSE, error=FALSE}
write_csv(df_owid,file = "data/processed/df_owid.csv")

write_csv(df_gapminder, file = "data/processed/df_gapminder.csv")
```
